<!doctype html>
<html>

<head>

  <title>
    Tanveer Khan
  </title>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf-8">

  <link rel="stylesheet" href="https://andrewcharlesjones.github.io/assets/css/main.css">
  <link rel="stylesheet" href="https://andrewcharlesjones.github.io/assets/css/syntax.css">
  <!-- Use Atom -->
  <link type="application/atom+xml" rel="alternate" href="https://tkhan11.github.io/" title="Tanveer Khan" />
  <!-- Use RSS-2.0 -->
  <!--<link href="https://tkhan11.github.io" type="application/rss+xml" rel="alternate" title="Tanveer Khan | CS PhD scholar @ Jamia Millia Islamia"/>
  //-->

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Code+Pro">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Quattrocento+Sans">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
    MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        }
      });
  </script>

  <!-- Google Analytics -->
  <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-149140730-1', 'auto');
  ga('send', 'pageview');
</script>


  <!-- Use Jekyll SEO plugin -->
  <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Fashion-MNIST Clothing Classification using Convolutional Neural Network (CNN)| Tanveer Khan</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Data Visualization in Python " />
<meta name="author" content="Tanveer Khan" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="layout: post title: “Data Visualization in Python ” author: “Tanveer Khan” categories: journal blurb: “” img: “” tags: [] —" />
<meta property="og:description" content="layout: post title: “Data Visualization in Python ” author: “Tanveer Khan” categories: journal blurb: “” img: “” tags: [] —" />
<link rel="canonical" href="https://tkhan11.github.io/post/page/3/project-7/index.html" />
<meta property="og:url" content="https://tkhan11.github.io/post/page/3/project-7/index.html" />
<meta property="og:site_name" content="Tanveer Khan" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-04-14T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Fashion-MNIST Clothing Classification using Convolutional Neural Network (CNN)" />


<!--
<script type="application/ld+json">
{"datePublished":"2021-04-14T00:00:00+00:00","url":"https://tkhan11.github.io/post/page/3/project-7/generalized-pca.html","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://tkhan11.github.io/post/page/3/project-7/generalized-pca.html"},"author":{"@type":"Person","name":"Tanveer Khan"},"@context":"https://schema.org"}</script>
 End Jekyll SEO tag -->


</head>



<body>

  <div class="container">
    <header class="masthead">
  <h3 class="masthead-title">
    <a href="https://tkhan11.github.io/">Tanveer Khan </a>
    <small class="masthead-subtitle"> CS PhD scholar@ Jamia Millia Islamia</small>

<!--
    <div class="menu">

  <nav class="social-icons">

    <a href="https://github.com/tkhan11" target="_blank"><i class="fa fa-github" aria-hidden="true"></i></a>

    <a href="https://www.suraiyajabin.in/tanveer-ahmed-khan" target="_blank"><i class="fa fa-graduation-cap" aria-hidden="true"></i></a>

    <a href="https://twitter.com/" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>

    <a>tanveer1910377@jmi.ac.in</a>

 </nav>



</div>
-->

  </h3>
</header>


    <div class="post-container">
      <h1 style="line-height: 1.2;">
  Fashion-MNIST Clothing Classification using Convolutional Neural Network (CNN)
</h1>

<p style="text-align: justify">The Fashion-MNIST clothing classification problem is a new standard dataset used in computer vision and deep learning.</p>

<p style="text-align: justify">Although the dataset is relatively simple, it can be used as the basis for learning and practicing how to develop, evaluate, and use deep convolutional neural networks for image classification from scratch. This includes how to develop a robust test harness
 for estimating the performance of the model, how to explore improvements to the model, and how to save the model and later load it to make predictions on new data.</p>

<h2 id="the-exponential-family">Tutorial Overview</h2>

<p>This tutorial is divided into four parts they are:</p>
<dl>

  <dd>1.  Fashion MNIST Clothing Classification</dd>
  <dd>2.  Model Evaluation Methodology</dd>
  <dd>3.  How to Develop a Model</dd>
  <dd>4.  How to Make Predictions</dd>
</dl>


  <h2 id="the-exponential-family">Fashion MNIST Clothing Classification</h2>
	<p style="text-align: justify">The Fashion-MNIST dataset is proposed as a more challenging replacement dataset for the MNIST dataset.</p>
<p>It is a dataset comprised of 60,000 small square 28×28 pixel grayscale images of items of 10 types of clothing, such as shoes, t-shirts, dresses, and more. The mapping of all 0-9 integers to class labels is listed below.</p>

<ul>
  <li>0: T-shirt/top</li>
  <li>1: Trouser</li>
  <li>2: Pullover</li>
  <li>3: Dress</li>
  <li>4: Coat</li>
  <li>5: Sandal</li>
  <li>6: Shirt</li>
  <li>7: Sneaker</li>
  <li>8: Bag</li>
  <li>9: Ankle boot</li>
</ul>

<p style="text-align: justify">It is a more challenging classification problem than MNIST
  and top results are achieved by deep learning convolutional neural networks with a classification accuracy of about 90% to 95% on the hold out test dataset.</p>
<p>The example below loads the Fashion-MNIST dataset using the Keras API and creates a plot of the first nine images in the training dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">matplotlib</span> <span class="k">import</span> <span class="n">pyplot</span>
<span class="kn">from</span> <span class="nn">keras.datasets</span> <span class="k">import</span> <span class="n">fashion_mnist</span>

<p>
# example of loading the fashion mnist dataset
# load dataset
(trainX, trainy), (testX, testy) = fashion_mnist.load_data()

# summarize loaded dataset
print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))
print('Test: X=%s, y=%s' % (testX.shape, testy.shape))

# plot first few images
for i in range(9):
  # define subplot
  pyplot.subplot(330 + 1 + i)
  # plot raw pixel data
  pyplot.imshow(trainX[i], cmap=pyplot.get_cmap('gray'))
# show the figure
pyplot.show()


# example of loading the fashion mnist dataset
from matplotlib import pyplot
from keras.datasets import fashion_mnist

# load dataset
(trainX, trainy), (testX, testy) = fashion_mnist.load_data()

# summarize loaded dataset
print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))
print('Test: X=%s, y=%s' % (testX.shape, testy.shape))

# plot first few images
for i in range(9):
  # define subplot
  pyplot.subplot(330 + 1 + i)
	# plot raw pixel data
	pyplot.imshow(trainX[i], cmap=pyplot.get_cmap('gray'))

# show the figure
pyplot.show()
</p>
</code></pre></div></div>



<p style="text-align: justify">Running the example loads the Fashion-MNIST train and test dataset and prints their shape.</p>
<p>We can see that there are 60,000 examples in the training dataset and 10,000 in the test dataset and that images are indeed square with 28×28 pixels..</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<p>

# record model performance on a validation dataset during training
history = model.fit(..., validation_data=(valX, valY))

# record model performance on a validation dataset during training
history = model.fit(..., validation_data=(valX, valY))
</p>
</code></pre></div></div>

<p><img src="/images/fashion.png" alt="fashion" /></p>


<h2 id="the-exponential-family">Model Evaluation Methodology</h2>

<p style="text-align: justify">The <a href="https://www.kaggle.com/zalando-research/fashionmnist/kernels">Fashion-MNIST</a> dataset was developed as a response to the wide use of the <a href ="http://yann.lecun.com/exdb/mnist/">MNIST dataset</a>, that has been effectively “solved” given the use of modern convolutional neural networks.</p>

<p style="text-align: justify">Fashion-MNIST was proposed to be a replacement for MNIST, and although it has not been solved, it is possible to routinely achieve error rates of 10% or less. Like MNIST, it can be a useful starting point for developing and practicing a methodology for solving image classification using convolutional neural networks.</p>

<p style="text-align: justify">Instead of reviewing the literature on well-performing models on the dataset, we can develop a new model from scratch.</p>

<p style="text-align: justify">The dataset already has a well-defined train and test dataset that we can use.</p>

<p style="text-align: justify">In order to estimate the performance of a model for a given training run, we can further split the training set into a train and validation dataset. Performance on the train and validation dataset over each run can then be plotted to provide learning curves and insight into how well a model is learning the problem.</p>

<p style="text-align: justify">The Keras API supports this by specifying the “validation_data” argument to the model.fit() function when training the model, that will, in turn, return an object that describes model performance for the chosen loss and metrics on each training epoch.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<p>

# record model performance on a validation dataset during training
history = model.fit(..., validation_data=(valX, valY))

# record model performance on a validation dataset during training
history = model.fit(..., validation_data=(valX, valY))
</p>
</code></pre></div></div>

<p style="text-align: justify">In order to estimate the performance of a model on the problem in general, we can use k-fold cross-validation, perhaps 5-fold cross-validation. This will give some account of the model’s variance with both respect to differences in the training and test datasets and the stochastic nature of the learning algorithm. The performance of a model can be taken as the mean performance across k-folds, given with the standard deviation, that could be used to estimate a confidence interval if desired.</p>

<p style="text-align: justify">We can use the KFold class from the scikit-learn API to implement the k-fold cross-validation evaluation of a given neural network model. There are many ways to achieve this, although we can choose a flexible approach where the KFold is only used to specify the row indexes used for each split.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<p>

# example of k-fold cv for a neural net
data = ...
# prepare cross validation
kfold = KFold(5, shuffle=True, random_state=1)
# enumerate splits
for train_ix, test_ix in kfold.split(data):
        model = ...
	...
</p>
</code></pre></div></div>


<p>We will hold back the actual test dataset and use it as an evaluation of our final model.</p>

<h2 id="the-exponential-family">How to Develop a Baseline Model</h2>

<p>The first step is to develop a baseline model.</p>

<p style="text-align: justify">This is critical as it both involves developing the infrastructure for the test harness so that any model we design can be evaluated on the dataset, and it establishes a baseline in model performance on the problem, by which all improvements can be compared.</p>

<p style="text-align: justify">The design of the test harness is modular, and we can develop a separate function for each piece. This allows a given aspect of the test harness to be modified or inter-changed, if we desire, separately from the rest.</p>

<p style="text-align: justify">We can develop this test harness with five key elements. They are the loading of the dataset, the preparation of the dataset, the definition of the model, the evaluation of the model, and the presentation of results.</p>


<h3 id="the-exponential-family">Load Dataset</h3>

<p style="text-align: justify">We know some things about the dataset.</p>

<p style="text-align: justify">For example, we know that the images are all pre-segmented (e.g. each image contains a single item of clothing), that the images all have the same square size of 28×28 pixels, and that the images are grayscale. Therefore, we can load the images and reshape the data arrays to have a single color channel.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<p>
# load dataset
(trainX, trainY), (testX, testY) = fashion_mnist.load_data()

# reshape dataset to have a single channel
trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))
testX = testX.reshape((testX.shape[0], 28, 28, 1))
</p>
</code></pre></div></div>

<p style="text-align: justify">We also know that there are 10 classes and that classes are represented as unique integers.</p>

<p style="text-align: justify">We can, therefore, use a one hot encoding for the class element of each sample, transforming the integer into a 10 element binary vector with a 1 for the index of the class value. We can achieve this with the to_categorical() utility function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<p>
# one hot encode target values
trainY = to_categorical(trainY)
testY = to_categorical(testY)
</p>
</code></pre></div></div>

<p style="text-align: justify">The load_dataset() function implements these behaviors and can be used to load the dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<p>
# load train and test dataset
def load_dataset():

# load dataset
(trainX, trainY), (testX, test)
fashion_mnist.load_data()

# reshape dataset to have a single channe
trainX = trainX.reshape((trainX.shap 28, 28, 1))
testX = testX.reshape((testX.shape[0], 28, 28, 1))

# one hot encode target values
trainY = to_categorical(trainY)

testY = to_categorical(testY)
return trainX, trainY, testX, test
</code></pre></div></div>

<h3 id="the-exponential-family">Prepare Pixel Data</h3>

<p style="text-align: justify">We know that the pixel values for each image in the dataset are unsigned integers in the range between black and white, or 0 and 255.</p>

<p style="text-align: justify">We do not know the best way to scale the pixel values for modeling, but we know that some scaling will be required.</p>

<p style="text-align: justify">A good starting point is to normalize the pixel values of grayscale images, e.g. rescale them to the range [0,1]. This involves first converting the data type from unsigned integers to floats, then dividing the pixel values by the maximum value.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<p>
# convert from integers to floats
train_norm = train.astype('float32')
test_norm = test.astype('float32')

<p style="text-align: justify"># normalize to rang
train_norm = train_norm / 255.0
test_norm = test_norm / 255.0
</p>
</code></pre></div></div>

<p style="text-align: justify">The prep_pixels() function below implement these behaviors and is provided with the pixel values for both the train and test datasets that will need to be scaled.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<p>
# scale pixels
def prep_pixels(train, test):
	# convert from integers to floats
	train_norm = train.astype('float32')
	test_norm = test.astype('float32')
	# normalize to range 0-1
  train_norm = train_norm .0
	test_norm = test_norm / 255.0
	# return normalized images
	return train_norm, test_norm
</p>>
</code></pre></div></div>

<p style="text-align: justify">This function must be called to prepare the pixel values prior to any modeling.</p>
<h3 id="the-exponential-family">Define Model</h3>

<p style="text-align: justify">Next, we need to define a baseline convolutional neural network model for the problem.</p>

<p style="text-align: justify">The model has two main aspects: the feature extraction front end comprised of convolutional and pooling layers, and the classifier backend that will make a prediction.</p>

<p style="text-align: justify">For the convolutional front-end, we can start with a single convolutional layer with a small filter size (3,3) and a modest number of filters (32) followed by a max pooling layer. The filter maps can then be flattened to provide features to the classifier.</p>

<p style="text-align: justify"><p style="text-align: justify">Given that the problem is a multi-class classification, we know that we will require an output layer with 10 nodes in order to predict the probability distribution of an image belonging to each of the 10 classes. This will also require the use of a softmax activation function. Between the feature extractor and the output layer, we can add a dense layer to interpret the features, in this case with 100 nodes.</p>

<p style="text-align: justify"><p style="text-align: justify">Aivation function and the He weight initialization scheme, both best practices.</p>

<p style="text-align: justify">We will use a conservative configuration for the stochastic gradient descent optimizer with a learning rate of 0.01 and a momentum of 0.9. The categorical cross-entropy loss function will be optimized, suitable for multi-class classification, and we will monitor the classification accuracy metric, which is appropriate given we have the same number of examples in each of the 10 classes.</p>

<p style="text-align: justify">The define_model() function below will define and return this model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<p>
# define cnn model
def define_model():
	model = Sequential()
	model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))
	model.add(MaxPooling2D((2, 2)))
	model.add(Flatten())
	model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))
	model.add(Dense(10, activation='softmax'))
	# compile model
	opt = SGD(lr=0.01, momentum=0.9)
	model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
	return model
</p>
</code></pre></div></div>

<h3 id="the-exponential-family">Evaluate Model</h3>

<p style="text-align: justify">After the model is defined, we need to evaluate it.</p>

<p style="text-align: justify">The model will be evaluated using 5-fold cross-validation. The value of k=5 was chosen to provide a baseline for both repeated evaluation and to not be too large as to require a long running time. Each test set will be 20% of the training dataset, or about 12,000 examples, close to the size of the actual test set for this problem.</p>

<p style="text-align: justify">The training dataset is shuffled prior to being split and the sample shuffling is performed each time so that any model we evaluate will have the same train and test datasets in each fold, providing an apples-to-apples comparison.</p>

<p style="text-align: justify">We will train the baseline model for a modest 10 training epochs with a default batch size of 32 examples. The test set for each fold will be used to evaluate the model both during each epoch of the training run, so we can later create learning curves, and at the end of the run, so we can estimate the performance of the model. As such, we will keep track of the resulting history from each run, as well as the classification accuracy of the fold.</p>

<p style="text-align: justify">The evaluate_model() function below implements these behaviors, taking the training dataset as arguments and returning a list of accuracy scores and training histories that can be later summarized.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<p>
# evaluate a model using k-fold cross-validation
def evaluate_model(dataX, dataY, n_folds=5):
	scores, histories = list(), list()
	# prepare cross validation
	kfold = KFold(n_folds, shuffle=True, random_state=1)
	# enumerate splits
	for train_ix, test_ix in kfold.split(dataX):
		# define model
		model = define_model()
		# select rows for train and test
		trainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]
		# fit model
		history = model.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY), verbose=0)
		# evaluate model
		_, acc = model.evaluate(testX, testY, verbose=0)
		print('> %.3f' % (acc * 100.0))
		# append scores
		scores.append(acc)
		histories.append(history)
	return scores, histories
</p>
</code></pre></div></div>

<h3 id="the-exponential-family">Present Results</h3>
<p style="text-align: justify">Once the model has been evaluated, we can present the results.</p>

<p style="text-align: justify">There are two key aspects to present: the diagnostics of the learning behavior of the model during training and the estimation of the model performance. These can be implemented using separate functions.</p>

<p style="text-align: justify">First, the diagnostics involve creating a line plot showing model performance on the train and test set during each fold of the k-fold cross-validation. These plots are valuable for getting an idea of whether a model is overfitting, underfitting, or has a good fit for the dataset.</p>

<p style="text-align: justify">We will create a single figure with two subplots, one for loss and one for accuracy. Blue lines will indicate model performance on the training dataset and orange lines will indicate performance on the hold out test dataset. The summarize_diagnostics() function below creates and shows this plot given the collected training histories.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<p>
# plot diagnostic learning curves
def summarize_diagnostics(histories):
	for i in range(len(histories)):
		# plot loss
		pyplot.subplot(211)
		pyplot.title('Cross Entropy Loss')
		pyplot.plot(histories[i].history['loss'], color='blue', label='train')
		pyplot.plot(histories[i].history['val_loss'], color='orange', label='test')
		# plot accuracy
		pyplot.subplot(212)
		pyplot.title('Classification Accuracy')
		pyplot.plot(histories[i].history['accuracy'], color='blue', label='train')
		pyplot.plot(histories[i].history['val_accuracy'], color='orange', label='test')
	pyplot.show()
</p>
</code></pre></div></div>

<p style="text-align: justify">Next, the classification accuracy scores collected during each fold can be summarized by calculating the mean and standard deviation. This provides an estimate of the average expected performance of the model trained on this dataset, with an estimate of the average variance in the mean. We will also summarize the distribution of scores by creating and showing a box and whisker plot.</p>

<p style="text-align: justify">The summarize_performance() function below implements this for a given list of scores collected during model evaluation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<p>
# summarize model performance
def summarize_performance(scores):
	# print summary
	print('Accuracy: mean=%.3f std=%.3f, n=%d' % (mean(scores)*100, std(scores)*100, len(scores)))
	# box and whisker plots of results
	pyplot.boxplot(scores)
	pyplot.show()
</p>
</code></pre></div></div>

<h3 id="the-exponential-family">Complete Example</h3>

<p style="text-align: justify">We need a function that will drive the test harness.</p>

<p style="text-align: justify">This involves calling all of the define functions.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<p>
# run the test harness for evaluating a model
def run_test_harness():
	# load dataset
	trainX, trainY, testX, testY = load_dataset()
	# prepare pixel data
	trainX, testX = prep_pixels(trainX, testX)
	# evaluate model
	scores, histories = evaluate_model(trainX, trainY)
	# learning curves
	summarize_diagnostics(histories)
	# summarize estimated performance
	summarize_performance(scores)
</p>
</code></pre></div></div>

<p style="text-align: justify">We now have everything we need; the complete code example for a baseline convolutional neural network model on the MNIST dataset is listed below.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<p>
# baseline cnn model for fashion mnist
from numpy import mean
from numpy import std
from matplotlib import pyplot
from sklearn.model_selection import KFold
from keras.datasets import fashion_mnist
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Conv2D
from keras.layers import MaxPooling2D
from keras.layers import Dense
from keras.layers import Flatten
from keras.optimizers import SGD

# load train and test dataset
def load_dataset():
	# load dataset
	(trainX, trainY), (testX, testY) = fashion_mnist.load_data()
	# reshape dataset to have a single channel
	trainX = trainX.reshape((trainX.shape[0], 28, 28, 1))
	testX = testX.reshape((testX.shape[0], 28, 28, 1))
	# one hot encode target values
	trainY = to_categorical(trainY)
	testY = to_categorical(testY)
	return trainX, trainY, testX, testY

# scale pixels
def prep_pixels(train, test):
	# convert from integers to floats
	train_norm = train.astype('float32')
	test_norm = test.astype('float32')
	# normalize to range 0-1
	train_norm = train_norm / 255.0
	test_norm = test_norm / 255.0
	# return normalized images
	return train_norm, test_norm

# define cnn model
def define_model():
	model = Sequential()
	model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(28, 28, 1)))
	model.add(MaxPooling2D((2, 2)))
	model.add(Flatten())
	model.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))
	model.add(Dense(10, activation='softmax'))
	# compile model
	opt = SGD(lr=0.01, momentum=0.9)
	model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
	return model

# evaluate a model using k-fold cross-validation
def evaluate_model(dataX, dataY, n_folds=5):
	scores, histories = list(), list()
	# prepare cross validation
	kfold = KFold(n_folds, shuffle=True, random_state=1)
	# enumerate splits
	for train_ix, test_ix in kfold.split(dataX):
		# define model
		model = define_model()
		# select rows for train and test
		trainX, trainY, testX, testY = dataX[train_ix], dataY[train_ix], dataX[test_ix], dataY[test_ix]
		# fit model
		history = model.fit(trainX, trainY, epochs=10, batch_size=32, validation_data=(testX, testY), verbose=0)
		# evaluate model
		_, acc = model.evaluate(testX, testY, verbose=0)
		print('> %.3f' % (acc * 100.0))
		# append scores
		scores.append(acc)
		histories.append(history)
	return scores, histories

# plot diagnostic learning curves
def summarize_diagnostics(histories):
	for i in range(len(histories)):
		# plot loss
		pyplot.subplot(211)
		pyplot.title('Cross Entropy Loss')
		pyplot.plot(histories[i].history['loss'], color='blue', label='train')
		pyplot.plot(histories[i].history['val_loss'], color='orange', label='test')
		# plot accuracy
		pyplot.subplot(212)
		pyplot.title('Classification Accuracy')
		pyplot.plot(histories[i].history['accuracy'], color='blue', label='train')
		pyplot.plot(histories[i].history['val_accuracy'], color='orange', label='test')
	pyplot.show()

# summarize model performance
def summarize_performance(scores):
	# print summary
	print('Accuracy: mean=%.3f std=%.3f, n=%d' % (mean(scores)*100, std(scores)*100, len(scores)))
	# box and whisker plots of results
	pyplot.boxplot(scores)
	pyplot.show()

# run the test harness for evaluating a model
def run_test_harness():
	# load dataset
	trainX, trainY, testX, testY = load_dataset()
	# prepare pixel data
	trainX, testX = prep_pixels(trainX, testX)
	# evaluate model
	scores, histories = evaluate_model(trainX, trainY)
	# learning curves
	summarize_diagnostics(histories)
	# summarize estimated performance
	summarize_performance(scores)

# entry point, run the test harness
run_test_harness()
</p>
</code></pre></div></div>

<p style="text-align: justify">Running the example prints the classification accuracy for each fold of the cross-validation process. This is helpful to get an idea that the model evaluation is progressing.</p>

<p style="text-align: justify">Note: Your results may vary given the stochastic nature of the algorithm or evaluation procedure, or differences in numerical precision. Consider running the example a few times and compare the average outcome.</p>

<p style="text-align: justify">We can see that for each fold, the baseline model achieved an error rate below 10%, and in two cases 98% and 99% accuracy. These are good results.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<p>
> 91.200
> 91.217
> 90.958
> 91.242
> 91.317
</p>
</code></pre></div></div>

<p style="text-align: justify">Next, a diagnostic plot is shown, giving insight into the learning behavior of the model across each fold.</p>

<p style="text-align: justify">In this case, we can see that the model generally achieves a good fit, with train and test learning curves converging. There may be some signs of slight overfitting.</p>

<p><img src="/images/acc.png" alt="accuracy" /></p>

<p style="text-align: justify">Next, the summary of the model performance is calculated. We can see in this case, the model has an estimated skill of about 96%, which is impressive.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<p>
Accuracy: mean=91.187 std=0.121, n=5
</p>
</code></pre></div></div>

<p style="text-align: justify">Finally, a box and whisker plot is created to summarize the distribution of accuracy scores.</p>

<p><img src="/images/boxplot.png" alt="boxplot" /></p>

<p style="text-align: justify">As we would expect, the distribution spread across the low-nineties.</p>

<p style="text-align: justify">We now have a robust test harness and a well-performing model.</p>


<!-- <span class="post-date">


  May
  10th,
  2021
  by

    Tanveer Khan

</span> -->

<!-- <div class="post-date"></div>
  <div class="sharing-icons">
    <a href="https://twitter.com/intent/tweet?text=Generalized Principal Component Analysis (PCA)&amp;url="https://tkhan11.github.io/post/page/3/project-7/index.html" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>
    <a href="https://www.facebook.com/sharer/sharer.php?u="https://tkhan11.github.io/post/page/3/project-7/index.html"&amp;title="Data Visualization in Python" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a>
  </div>
</div>
 -->

<!-- <div class="related">
  <h1 ></h1>

  <ul class="related-posts">

  </ul>
</div>
 -->



    </div>

    <footer class="footer">
  <!--


    <a href="https://github.com/tkhan11" target="_blank"><i class="fa fa-github" aria-hidden="true"></i></a>




    <a href="https://suraiyajabin.in/tanveer-ahmed-khan" target="_blank"><i class="fa fa-graduation-cap" aria-hidden="true"></i></a>




    <a href="https://twitter.com/" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>




    <a>tanveer1910377@jmi.ac.in</a>

 -->
  <div class="post-date"><a href="https://tkhan11.github.io/about/">Tanveer Khan | CS PhD scholar @ Jamia Millia Islamia</a></div>
</footer>

  </div>

</body>
</html>

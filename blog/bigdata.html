<!doctype html>
<html>

<head>

  <title>
    Tanveer Ahmed Khan D4DI
  </title>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="utf-8">

  <link rel="stylesheet" href="/dist/css/main.css">
  <link rel="stylesheet" href="/dist/css/syntax.css">
  <link type="application/atom+xml" rel="alternate" href="https://tkhan11.github.io/" title="Tanveer Khan" />
  <!-- Use RSS-2.0 -->
  <!--<link href="https://tkhan11.github.io" type="application/rss+xml" rel="alternate" title="Tanveer Khan | CS PhD scholar @ Jamia Millia Islamia"/>
  //-->

  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=PT+Serif:400,400italic,700%7CPT+Sans:400">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Source+Code+Pro">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Quattrocento+Sans">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">

  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
    MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [['$', '$'], ['\\(', '\\)']]
        }
      });
  </script>

 <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-PND0KQF3PH"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-PND0KQF3PH');
</script>


  <!-- Use Jekyll SEO plugin -->
  <!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Superstore sales data analysis using Big Data Hadoop framework | Tanveer Khan</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Superstore sales data analysis using Big Data Hadoop framework" />
<meta name="author" content="Tanveer Khan" />
<meta name="keywords" content="D4DI, Data4DerivingInsights">
<meta property="og:locale" content="en_US" />
<meta name="description" content="layout: post title: “Superstore sales data analysis using Big Data Hadoop framework” author: “Tanveer Khan” categories: journal blurb: “” img: “” tags: [] —" />
<meta property="og:description" content="layout: post title: “Superstore sales data analysis using Big Data Hadoop framework  ” author: “Tanveer Khan” categories: journal blurb: “” img: “” tags: [] —" />
<link rel="canonical" href="https://tkhan11.github.io/blog/bigdata.html" />
<meta property="og:url" content="https://tkhan11.github.io/blog/bigdata.html" />
<meta property="og:site_name" content="Tanveer Khan" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-05-14T00:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Superstore sales data analysis using Big Data Hadoop framework " />



<script type="application/ld+json">
{"datePublished":"2021-05-14T00:00:00+00:00","url":"https://tkhan11.github.io/blog/pca.html","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://tkhan11.github.io/blog/bigdata.html"},"author":{"@type":"Person","name":"Tanveer Khan"},"@context":"https://schema.org"}</script>
 <!--End Jekyll SEO tag -->


</head>



<body>

  <div class="container">
    <header class="masthead">
  <h3 class="masthead-title">
    <a href="https://tkhan11.github.io/">Tanveer Khan </a>
    <small class="masthead-subtitle"> Data Scientist @ NextGen Invent | Research Scholar @ Jamia Millia Islamia</small>


   <div class="menu">

     <nav class="menu-content">

         <a href="https://tkhan11.github.io/blog/blog.html">Blog</a>

         <a href="https://tkhan11.github.io/publications/">Research</a>

       <!--  <a href="https://tkhan11.github.io/publications/files/Tanveer_Khan_CV.pdf">CV</a>
     -->
     </nav>

  <nav class="social-icons">

    <a href="https://github.com/tkhan11" target="_blank"><i class="fa fa-github" aria-hidden="true"></i></a>

    <a href="https://www.suraiyajabin.in/tanveer-ahmed-khan" target="_blank"><i class="fa fa-graduation-cap" aria-hidden="true"></i></a>

    <a href="https://twitter.com/" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>

    <a>tanveerkhand4di@gmail.com</a>

 </nav>



</div>

  </h3>
</header>


    <div class="post-container">
      <h1 style="line-height: 1.2;">
Superstore sales data analysis using Big Data Hadoop framework
</h1>

<p>
<img src="/images/bigdata.png" alt="bigdata">
</p>


<p style="text-align: justify">Big Data Analytics refers to the strategy of analyzing large volumes of data, or big data. This big data is gathered from a wide variety of sources, including social networks, videos, digital images, log files, sensor data, and sales transaction records, etc.
   The aim is to analyze all this data to discover patterns, findings, and trends that will help the concerned stakeholders to take informed decisions.</p>



<h2 id="the-exponential-family">Objectives</h2>

<p style="text-align: justify"> The general and specific objectives are being discussed in this section.</p>

<h3 id="the-exponential-family">General objective:</h3>
<p style="text-align: justify">The main objective here is to use big data hadoop framework for analyzing superstore
  sales data and to design a Graphical user interface (GUI) for the same.</p>

<h3 id="the-exponential-family">Specific objectives:</h3>
<ul>
  <li> To design a user interface which will interact with HDFS a, distributed data warehouse using gateway node.</li>
  <li> To produce MapReduce jobs and HIVE queries for the analysis of the superstore data file.</li>
  <li> To install the Hadoop v2.7.5 in standalone mode together with all its frameworks and applications for the analysis purpose.</li>
</ul>

<h2 id="the-exponential-family">Dataset</h2>

<p style="text-align: justify"><a href="https://www.kaggle.com/juhi1994/superstore">Superstore Sales</a> - This dataset has transactions records of US customers from years 2014-2018
for an E-commerce platform that allows people to buy products from books, toys, clothes, and shoes to food, furniture, and other household items.</p>

<h2 id="the-exponential-family">Motivation</h2>
<p style="text-align: justify"> Our motivation lies in exploring superstore dataset for finding answers to these questions.
<ul>
  <li> How much profit is gained for each product.?</li>
  <li> People from city/state shop the most?</li>
  <li> Average total sales purchase price per cutomer.?</li>
  <li> Finding category wise total sales price.?</li>
  <li> Customer total purchase price.?</li>
  <li> Total Profit attained by an individual customer.?</li>
  <li> State region wise total sales price.?</li>
  <li> Total number of transactions by an individual customer.?</li>
  <li> Quantity based analysis.?</li>
</ul>


<h2 id="the-exponential-family">Requirements</h2>
<h3 id="the-exponential-family">Hardware requirements:</h3>
<ul>
  <li> System with i3 processor or later</li>
  <li> System with Minimum 4GB RAM and minimum 20GB Hard Drive.</li>
  <li> System having Virtualization Technology (in case if using windows).</li>
</ul>

<h3 id="the-exponential-family">Software requirements:</h3>
<ul>
  <li> Operating System:Windows 7 or later or Linux v12.04 or later.</li>
  <li> VMware-workstation-full-14.1</li>
  <li> Cloudera-quickstart-vm-4.7.0-0-vmware</li>
  <li> Hadoop-2.9.0.tar, apache-hive-1.2.2-bin.tar.gz, sqoop-1.4.7.bin_hadoop- 2.6.0.tar.gz, pig-0.16.0-src.tar.gz</li>
  <li> Java development toolkit: jdk-8u152-linux-x64.tar.</li>
  <li> netbeans-8.2-windows for design and development purpose.</li>
</ul>

<h2 id="the-exponential-family">Methodology</h2>
<p style="text-align: justify">The proposed methodology for deisgn and development of Graphical user interface (GUI) and writing MapReduce jobs and HIVE queries for analysis pupose have been discussed here.</p>


<h3 id="the-exponential-family">Building a graphical user interface (GUI)</h3>
<p style="text-align: justify">For this purpose Netbeans software was used for designing a GUI. The figures below shows the flow chart and data flow diagram of the build system.</p>
<p>
  Flow chart of the proposed system.
  <img src="/images/flowchart.png" alt="flowchart">
</p>

<p>
  Data flow Diagram of the proposed system.
  <img src="/images/DFD.png" alt="DFD">
</p>

<p style="text-align: justify">Figures below shows the developed GUI for the proposed system. For the purpose of analyzing superstore dataset the developed GUI has screens for
authorization purpose, Execution of MapReduce jobs, generating reports, and saving the reports to the local system.</p>


<p>
  Login screen design for authorization purpose.<br><br>
  <img src="/images/login.png" alt="login">
</p>

<p>
  Screen design for generating MapReduce jobs.<br>
  <img src="/images/screen.png" alt="screen">
</p>

<p>
  Screen design for for generating reports.<br><br>
  <img src="/images/report.png" alt="report ">
</p>

<p style="text-align: justify"> The soucre code for the design and development of the proposed sytem system can be found  <a href="https://github.com/tkhan11/Big-Data-Hadoop-Project/tree/master/HadoopProjectMain/src">here</a>.</p>

<h3 id="the-exponential-family">Single Node Hadoop Installation on Linux v16.04:</h3>
<h4 id="the-exponential-family">Installing Java</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<p>
:~$ cd ~
# Update the source list
~$ sudo apt-get update

# The OpenJDK project is the default version of Java
# that is provided from a supported Ubuntu repository.
:~$ sudo apt-get install default-jdk
:~$ java -version
java version "1.7.0_65"

OpenJDK Runtime Environment (IcedTea 2.5.3) (7u71-2.5.3-0ubuntu0.14.04.1)
OpenJDK 64-Bit Server VM (build 24.65-b04, mixed mode)
</p>
</code></pre></div></div>


<h4 id="the-exponential-family">Adding a dedicated Hadoop user</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<p>
  :~$ sudo addgroup hadoop
  Adding group `hadoop' (GID 1002) ... Done.
  :~$ sudo adduser --ingroup hadoop hduser
  Adding user `hduser' ...
  Adding new user `hduser' (1001) with group `hadoop' ...
  Creating home directory `/home/hduser' ...
  Copying files from `/etc/skel' ...
  Enter new UNIX password
  Retype new UNIX password:
  passwd: password updated successfully
  Changing the user information for hduser
  Enter the new value, or press ENTER for the default
  Full Name []:
  Room Number []:
  Work Phone []:
  Home Phone []:
  Other []:
  Is the information correct? [Y/n] Y
</p>
</code></pre></div></div>

<h4 id="the-exponential-family">Install Hadoop</h4>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<p>
  :~$ wget http://mirrors.sonic.net/apache/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz
  :~$ tar xvzf hadoop-2.6.0.tar.gz
  :/home/hduser$ sudo su hduser
  :~/hadoop-2.6.0$ sudo mv * /usr/local/hadoop
  :~/hadoop-2.6.0$ sudo chown -R hduser:hadoop /usr/local/hadoop
</p>
</code></pre></div></div>

<p style="text-align: justify">The following files will have to be modified to complete the Hadoop setup:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<p>
~/.bashrc
/usr/local/hadoop/etc/hadoop/hadoop-env.sh
/usr/local/hadoop/etc/hadoop/core-site.xml
/usr/local/hadoop/etc/hadoop/mapred-site.xml.template
/usr/local/hadoop/etc/hadoop/hdfs-site.xml
</p>
</code></pre></div></div>

<p style="text-align: justify">1. ~/.bashrc: <br>Before editing the .bashrc file in our home directory, we need to find the path where Java has been
installed to set the JAVA_HOME environment variable using the following command.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<p>
hduser@laptop update-alternatives --config java
There is only one alternative in link group java (providing /usr/bin/java):/usr/lib/jvm/java-7-openjdk-amd64/jre/bin/java
Nothing to configure.

# Now we can append the following to the end of ~/.bashrc:

hduser@laptop:~$ vi ~/.bashrc
#HADOOP VARIABLES START
export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64
export HADOOP_INSTALL=/usr/local/hadoop
export PATH=$PATH:$HADOOP_INSTALL/bin
export PATH=$PATH:$HADOOP_INSTALL/sbin
export HADOOP_MAPRED_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_HOME=$HADOOP_INSTALL
export HADOOP_HDFS_HOME=$HADOOP_INSTALL
export YARN_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_INSTALL/lib"

#HADOOP VARIABLES END
hduser@laptop:~$ source ~/.bashrc
</p>
</code></pre></div></div>

<p style="text-align: justify">2. Editing  the /usr/local/hadoop/etc/hadoop/hadoop-env.sh. <br> We need to set JAVA_HOME by modifying hadoop-env.sh file.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<p>
hduser@laptop:~$ vi /usr/local/hadoop/etc/hadoop/hadoop-env.sh
export JAVA_HOME=/usr/lib/jvm/java-7-openjdk-amd64
</p>
</code></pre></div></div>

<p style="text-align: justify">3. Editing  the/usr/local/hadoop/etc/hadoop/core-site.xml: <br>The /usr/local/hadoop/etc/hadoop/core-site.xml file contains configuration properties that Hadoop uses when starting up. This file can be used to override the default settings that Hadoop starts with.
Open the file and enter the following in between the <configuration></configuration> tag:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<p>
hduser@laptop:~$ vi /usr/local/hadoop/etc/hadoop/core-site.xml
<configuration>
<property>
<name>hadoop.tmp.dir</name>
<value>/app/hadoop/tmp</value>
<description>A base for other temporary directories.</description>
</property>
</configuration>
</p>
</code></pre></div></div>


<p style="text-align: justify">4. Editing the mapred-site.xml.template file. <br>By default the /usr/local/hadoop/etc/hadoop/ folder contains /usr/local/hadoop/etc/hadoop/mapred-site.xml.template. The mapred-site.xml file is used to specify which framework is being used for MapReduce. We need to enter the following content in between the <configuration></configuration> tag:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<p>
<configuration>
<property>
<name>mapred.job.tracker</name>
<value>localhost:54311</value>
<description>The host and port that the MapReduce job tracker runs
at. If "local", then jobs are run in-process as a single map
and reduce task.
</description>
</property>
</configuration>
</p>
</code></pre></div></div>



<p style="text-align: justify">5. Editing the /usr/local/hadoop/etc/hadoop/hdfs-site.xml. <br>The /usr/local/hadoop/etc/hadoop/hdfs-site.xml file needs to be configured for each host in the cluster that is being used. It is used to specify the directories which will be used as the namenode and the datanode on that host. Before editing this file, we need to create two directories which will contain the namenode and the datanode for this Hadoop installation. Open the file and enter the following content in between the <configuration></configuration> tag:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<p>
hduser@laptop:~$ vi /usr/local/hadoop/etc/hadoop/hdfs-site.xml
<configuration>
<property>
<name>dfs.replication</name>
<value>1</value>
</property>
<property>
<name>dfs.namenode.name.dir</name>
<value>file:/usr/local/hadoop_store/hdfs/namenode</value>
</property>
<property>
<name>dfs.datanode.data.dir</name>
<value>file:/usr/local/hadoop_store/hdfs/datanode</value>
</property>
</configuration>
</p>
</code></pre></div></div>

<p style="text-align: justify">Format the New Hadoop Filesystem. Now, the Hadoop file system needs to be formatted so that we can start to use it.
  The format command should be issued with write permission since it creates current directory under /usr/local/hadoop_store/hdfs/namenode folder. After this we are good to go..!!!</p>

<h2 id="the-exponential-family">Conclusion</h2>
<p style="text-align: justify">The Proposed system will provide us with the Graphical User interface (GUI) for performing the execution of MapReduce jobs efficiently
  and in an easy way by analyzing large dataset on Big Data Hadoop single node cluster. Additionally, proposed system will act as baseline for other datasets as we
  can modify it with the reports we want to execute by producing MapReduce Jobs for the same.</p>

  <h2 id="references">References</h2>

  <ul  style="text-align: justify">

  <li>Saldhi, A., Yadav, D., Saksena, D., Goel, A., Saldhi, A., & Indu, S. (2014, December). Big data analysis using Hadoop cluster. In 2014 IEEE International Conference on Computational Intelligence and Computing Research (pp. 1-6). IEEE.</li>

  <li>Nayak, B. (2020). Hadoop Internals and Big Data Evidence. In Big Data Analytics and Computing for Digital Forensic Investigations (pp. 65-86). CRC Press.</li>

  <li>Kumar, N. (2021). Big Data Using Hadoop and Hive. Stylus Publishing, LLC.</li>

  </ul>


    </div>

    <footer class="footer">
  <!--


    <a href="https://github.com/tkhan11" target="_blank"><i class="fa fa-github" aria-hidden="true"></i></a>




    <a href="https://suraiyajabin.in/tanveer-ahmed-khan" target="_blank"><i class="fa fa-graduation-cap" aria-hidden="true"></i></a>




    <a href="https://twitter.com/" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a>




    <a>tanveer1910377@jmi.ac.in</a>

 -->
  <div class="post-date"><a href="https://tkhan11.github.io/about/">Tanveer Khan | Data Scientist @ NextGen Invent | Research Scholar @ Jamia Millia Islamia</a></div>
</footer>

  </div>

</body>
</html>
